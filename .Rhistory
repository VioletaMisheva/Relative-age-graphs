X=data$X[index]
Y=data$Y[index]
return((var(Y)-cov(X,y))/(var(X)+var(Y)-2*cov(X,Y)))
}
alpha.fn(Portfolio, 1:100)
alpha.fn=function(data, index){
X=data$X[index]
Y=data$Y[index]
return((var(Y)-cov(X,Y))/(var(X)+var(Y)-2*cov(X,Y)))
}
alpha.fn(Portfolio, 1:100)
set.seed(1)
alpha.fn(Portfolio, sample(100, 100, replace=T))
boot(Portfolio, alpha.fn, R=1000)
boot.fn=function(data, index)
return(coef(lm(mpg~horsepower, data=data, subset=index)))
boot.fn(Auto, 1:392)
library(ISLR)
data(Default)
head(Default)
attach(Default)
glm.fit=glm(Default~income+balance, data=Default, family=binomial)
glm.fit=glm(Default~income+balance, data=Default, family="binomial")
glm.fit=glm(default~income+balance, data=Default, family="binomial")
set.seed(1)
glm.fit=glm(default~income+balance, data=Default, family="binomial")
summary(glm.fit)
FiveB=fn(){
FiveB=function(){
train=sample(dim(Default)[1], dim(Default)[1]/2)
glm.fit=glm(default~income+balance, data=Default, family=binomial, subset=train)
glm.pred = rep("No", dim(Default)[1]/2)
glm.probs = predict(glm.fit, Default[-train, ], type = "response")
glm.pred[glm.probs > 0.5] = "Yes"
return(mean(glm.pred != Default[-train, ]$default))
}
FiveB()
FiveB()
FiveB()
FiveB()
library(ISLR)
data(Weekly)
names(Weekly)
set.seed(1)
attach(Weekly)
glm.fit = glm(Direction ~ Lag1 + Lag2, data = Weekly, family = binomial)
summary(glm.fit)
glm.fit = glm(Direction ~ Lag1 + Lag2, data = Weekly[-1, ], family = binomial)
summary(glm.fit)
predict.glm(glm.fit, Weekly[1, ], type = "response") > 0.5
Direction[1 ,]
names(Weekly)
Weekly[1 ,]$Direction
count = rep(0, dim(Weekly)[1])
for (i in 1:(dim(Weekly)[1])) {
glm.fit = glm(Direction ~ Lag1 + Lag2, data = Weekly[-i, ], family = binomial)
is_up = predict.glm(glm.fit, Weekly[i, ], type = "response") > 0.5
is_true_up = Weekly[i, ]$Direction == "Up"
if (is_up != is_true_up)
count[i] = 1
}
sum(count)
sum(count)
library(ISLR)
names(Hitters)
Hitters=na.omit(Hitters)
dim(Hitters)
library(leaps)
install.packages("leaps")
library(leaps)
refit.full=regsubsets(Salary~., Hitters)
summary(refit.full)
regfit.full=regsubsets(Salary~., Hitters, nvmax=19)
reg.summary=summary(regfit.full)
reg.summary$rsq
par(mfrow=c(2,2))
plot(reg.summary$rss, xlab="Number of variables", y="RSS", type="j")
plot(reg.summary$rss, xlab="Number of variables", y="RSS", type="l")
plot(19, reg.summary$rss, xlab="Number of variables", y="RSS", type="l")
plot(reg.summary$rss, xlab="Number of variables", y="RSS", type="l")
reg.summary$rss
plot(reg.summary$rss , type="l")
plot(reg.summary$rss, xlab="Number of variables",   type="l")
plot(reg.summary$rss, xlab="Number of variables", ylab="RSS" , type="l")
par(mfrow=c(2,2))
plot(reg.summary$rss, xlab="Number of variables", ylab="RSS" , type="l")
plot(reg.summary$adjr2, xlab="Number of variables", ylab="Adjusted R2" , type="l")
which.max(reg.summary$adjr2)
points(11,reg.summary$adjr2[11], col="red", cex=2, pch=20)
plot(reg.summary$cp, xlab="Number of variables", ylab="Cp" , type="l")
plot(reg.summary$bic, xlab="Number of variables", ylab="BIC" , type="l")
plot(regfit.full, scale="r2")
plot(regfit.full, scale="r2")
plot(regfit.full, scale="Cp")
plot(regfit.full, scale="bic")
regfit.fw=regsubsets(Salary~., Hitters, nvmax=19, method="forward")
summary(regfit.fw)
regfit.bwd=regsubsets(Salary~., Hitters, nvmax=19, method="backward")
summary(regfit.bwd)
coef(regit.full, 7)
coef(regfit.full, 7)
coef(regfit.fwd, 7)
coef(regfit.fw, 7)
set.seed(!)
set.seed(1)
train=sample(c(TRUE, FALSE), nrow(Hitters), rep=TRUE)
test=(!train)
regfit.best=regsubsets(Salary~., data=Hitters[train,], nvmax=19)
test.mat=model.matrix(Salary~., data=Hitters[test,])
val.errors=rep(NA, 19)
for(i in 1:19){
coefi=coef(regfit.best, id=i)
pred=test.mat[, names(coefi)]%*%coefi
val.errors[i]=mean((Hitters$Salary[test]-pred)^2)
}
val.erros
val.errors
which.min(val.errors)
coef(regfit.best, 10)
names(Hitters)
x=model.matrix(Salary~., Hitters)[-, 1]
x=model.matrix(Salary~., Hitters)[, -1]
y=Hitters$Salary
library(glmnet)
install.packages("glmnet")
library(glmnet)
grid=10^seq(10, -2, length=100)
ridge.mod=glmnet(x,y,alpha=0, lambda=grid)
dim(coef(ridge.mod))
set.seed(1)
cv.out=cv.glmnet(x[train,], y[train], alpha=0)
plot(cv.out)
bestlam=cv.out$lambda.min
bestlam
out=glmnet(x, y, alpha=0)
predict(out, type="coefficients", s=beslam)[1:20 ,]
predict(out, type="coefficients", s=bestlam)[1:20 ,]
predict(out, type="coefficients", s=bestlam)
lasso.mod=glmnet(x[train,], y[train], alpha=1, lambda=grid)
plot(lasso.mod)
set.seed(1)
cv.out=cv.glmnet(x[train,], y[train], alpha=1)
bestlam=cv.out$lambda.min
lasso.pred=predict(lasso.mod, s=bestlam, newx=x[test ,])
mean((lasso.pred-y.test)^2)
mean((lasso.pred-y[test])^2)
y.test=y[test]
mean((lasso.pred-y.test)^2)
grid=10^seq(10,-2, length=100)
set.seed(1)
cv.out=cv.glmnet(x[train,], y[train], alpha=1)
bestlam=cv.out$lambda.min
lasso.pred=predict(lasso.mod, s=bestlam, newx=x[test ,])
mean((lasso.pred-y.test)^2)
lasso.mod=glmnet(x[train,], y[train], alpha=1, lambda=grid)
lasso.pred=predict(lasso.mod, s=bestlam, newx=x[test ,])
mean((lasso.pred-y.test)^2)
out=glmnet(x,y,alpha=1, lambda=grid)
lasso.coef=predict(out, type="coefficients", s=bestlam)[1:20 ,]
lasso.coef
install.packages("pls")
library(pls)
set.seed(2)
Hitters=na.omit(Hitter)
Hitters=na.omit(Hitters)
pcr.fit=pcr(Salary~., data=Hitters, scale=TRUE, validation="CV")
summary(pcr.fit)
validationplot(pcr.fit, val.type="MSEP")
set.seed(1)
pcr.fit=pcr(Salary~., data=Hitters, subset=train, scale=TRUE, validation="CV")
validationplot(pct.fit, va.type="MSEP")
validationplot(pct.fit, val.type="MSEP")
validationplot(pcr.fit, val.type="MSEP")
pcr.pred=predict(pcr.fit, x[test,], ncomp=7)
mean((pcr.pred-y.test)^2)
pcr.fit=pcr(y~x, scale=TRUE, ncomp=7)
summary(pcr.fit)
set.seed(1)
pls.fit=plsr(Salary~., data=Hitters, subset=train, scale=TRUE, validation="CV")
summary(pls.fit)
validationplot(pls.fit, val.type="MSEP")
set.seed(1)
X = rnorm(100)
eps = rnorm(100)
beta0 = 3
beta1 = 2
beta2 = -3
beta3 = 0.3
Y = beta0 + beta1 * X + beta2 * X^2 + beta3 * X^3 + eps
library(leaps)
data=data.frame(y=Y, x=X)
mod=regsubsets(y~pol(x, 10), data, nvmax=10)
mod=regsubsets(y~poly(x, 10), data, nvmax=10)
summary(mod)
summary.mod=summary(mod)
which.min(summary.mod)$cp
which.min(summary.mod$cp)
which.min(summary.mod$bic)
which.max(summary.mod$r2)
which.max(summary.mod$adjr2)
coefficients(mod, 3)
coefficients(mod, 4)
coefficients(mod, 5)
mod=regsubsets(y~poly(x, 10, raw=T), data, nvmax=10)
summary.mod=summary(mod)
which.max(summary.mod$adjr2)
mod.fw=regsubsets(y~poly(x, 10, raw=T), data, nvmax=10, method="forward")
mod.bw=regsubsets(y~poly(x, 10, raw=T), data, nvmax=10, method="backward")
summary.fwd=summary(mod.fw)
summary.bwd=summary(mod.bw)
which.min(summary.fwd$cp)
which.min(summary.bwd$cp)
library(glmnet)
xmat = model.matrix(y ~ poly(x, 10, raw = T), data = data)[, -1]
mod.lasso=cv.glmnet(xmat, y, alpha=1)
mod.lasso=cv.glmnet(xmat, Y, alpha=1)
bestlam=mod.lasso$lambda.min
plot(mod.lasso)
best.model = glmnet(xmat, Y, alpha = 1)
predict(best.model, s = best.lambda, type = "coefficients")
predict(best.model, s = bestlam, type = "coefficients")
library(ISLR)
attach(Wage)
fit=lm(Wage~poly(age, 4), data=Wage)
fit=lm(wage~poly(age, 4), data=Wage)
coef(summary(fit))
fit2=lm(wage~poly(age, 4, raw=TRUE), data=Wage)
coef(summary(fit2))
agelims=range(age)
agelims
age.grid=seq(from=agelims[1], to=agelims[2])
preds=predict(fit, newdata=list(age=age.grid), se=TRUE)
se.bands=cbind(preds$fit+2*preds$se.fit, preds$fit-2*preds$se.fit)
se.bands
par(mfrow=c(1,2), mar=c(4.5, 4.5, 1, 1), oma=c(0,0,4,0))
plot(age, wage, xlim=agelims, cex=0.5, col="darkgrey")
title("Degree-4 polynomial", outer=T)
lines(age.grid, preds$fit, lwd=2, col="blue")
matlines(age.grid, se.bands, lwd=1, col="blue", lty=3)
table(cut(age, 4))
fit=lm(wage~cut(age, 4), data=Wage)
coef(summary(fit))
library(splines)
fit=lm(wage~bs(age, knots=c(25, 40, 60)), data=Wage)
pred=predict(fit, newdata=list(age=age.grid), se=T)
plot(age, wage, col="grey")
lines(age.grid, pred$fit, lwd=2)
lines(age.grid, pred$fit+2*pred$se, lty="dashed")
lines(age.grid, pred$fit-2*pred$se, lty="dashed")
age.grid
fit4=loess(age~wage, span=0.2, data=Wage)
summary(fit4)
coef(summary(fit4))
library(gam)
install.packages("gam")
library(gam)
gam.m3=gam(wage~s(year, 4)+s(age, 5)+education, Wage)
gam.m3=gam(wage~s(year, 4)+s(age, 5)+education, data=Wage)
par(mfrow=c(1,3))
plot(gm.m3, se=TRUE, col="blue")
plot(gam.m3, se=TRUE, col="blue")
summary(gam.m3)
library(shiny)
library(trees)
install.packages("trees")
library(ISLR)
attach(Carseats)
head(cARSEATS)
head(Carseats)
summary(Sales)
High=ifelse(Sales<=8, "No", "Yes")
Carseats=data.frame(Carseats, High)
tree.carseats=tree(High~-Sales, Carseats)
library(trees)
library(tree)
install.packages("tree")
library(tree)
tree.carseats=tree(High~-Sales, Carseats)
summary(tree.carseats)
View(Carseats)
tree.carseats=tree(High~ . -Sales, Carseats)
summary(tree.carseats)
plot(tree.carseats)
text(tree.carseats)
text(tree.carseats, pretty)
plot(tree.carseats)
text(tree.carseats, pretty=0)
plot(tree.carseats)
text(tree.carseats, pretty=1)
tree.carseats
set.seed(2)
train=sample(1:nrow(Carseats), 200)
Carseats.test=Carseats[-train,]
High.test=High[-train]
tree.carseats=tree(High~. -Sales, Carseats, subset=train)
tree.pred=predict(tree.carseats, Carseats.test,type="class")
table(test.pred, High.test)
table(tree.pred, High.test)
(86+57)/200
set.seed(3)
cv.carseats=cv.tree(tree.carseats, FUN=prune.misclass)
cv.carseats
par(mfrow=c(1,2))
plot(cv.carseats$size, cv.carseats$dev, type="b")
plot(cv.carseats$k, cv.carseats$dev, type="b")
prune.carseats=prune.misclass(tree.carseats,best=9)
plot(prune.carseats)
text(prune.carseats, pretty=0)
tree.pred=predict(prune.carseats, Carseats.test, type="class")
table(tree.pred, High.test)
library(MASS)
set.seed(1)
train=sample(1:nrow(Boston), nrow(Boston)/2)
tree.boston=tree(medv~., Boston, subset=train)
summary(tree.boston)
plot.(boston.tree)
plot.(tree.boston)
plot(tree.boston)
plot(tree.boston)
text(tree.boston, pretty=0)
library(randomForest)
set.seed(1)
bag.boston=randomForest(medv~., data=Boston, subset=train, mtry=13, importance=TRUE)
bag.boston
set.seed(1)
rf.boston=randomForest(medv~., data=Boston, subset=train, mtry=6, importance=TRUE)
yhat.rf=predict(rf.boston, newdata=Boston[-train,])
mean((yhat.rf-boston.test)^2)
boston.test=Boston[-train, "medv"]
mean((yhat.rf-boston.test)^2)
importance(rf.boston)
varImpPlot(rf.boston)
install.packages("gbm")
library(gbm)
set.seed(1)
boost.boston=gbm(medv~., data=Boston[train, ], distribution="gaussian", n.trees=5000, interaction.depth=4)
summary(boost.boston)
summary(boost.boston)
par(mfrow=c(1,2))
plot(boost.boston, i="rm")
plot(boost.boston, i="lstat")
library(e1071)
set.seed(1)
x=matrix(rnorm(20*2),ncol=2)
x
y=c(rep(-1, 10), rep(1,10))
y
x[y==1,]=x[y==1,]+1
plot(x, col=(3-y))
dat=data.frame(x, as.factor(y))
library(e1071)
svmfit=svm(y~., data=dat, kernel="linear", cost=10, scale=FALSE)
plot(svmfit,dat)
plot(svmfit, dat)
plot(svmfit, dat)
View(dat)
plot(svmfit, dat)
svmfit
svmfit$index
dat=data.frame(x=x, y=as.factor(y))
svmfit=svm(y~., data=dat, kernel="linear", cost=10, scale=FALSE)
svmfit
svmfit$index
plot(svmfit, dat)
View(dat)
summary(svmfit)
set.seed(1)
tune.out=tune(svm, y~., data=dat, kernel="linear", ranges=list(cost=c(0.001,0.01, 0.1, 1,5, 10, 100)))
summary(tune.out)
bestmod=tune.out@best.model
bestmod=tune.out$best.model
summary(bestmod)
install.packages("ROCR")
library(ISLR)
names(Khan)
dim(Khan$xtrain)
dim(Khan$xtest)
length(Khan$ytrain)
length(Khan$ytest)
table(Khan$ytrain)
table(Khan$ytest)
dat=data.frame(x=Khan$xtrain, y=as.factor(Khan$ytrain))
out=svm(y~., data=dat, kernel="linear", cost=10)
summary(out)
table(out$fitted, data$y)
table(out$fitted, dat$y)
dat.te=data.frame(x=Khan$xtest, y=as.factor(Khan$ytest))
pred.te=predict(out, newdata=dat.te)
table(pred.te, dat.te$y)
states=row.names(USArrests)
states
apply(USArrests,2,mean)
apply(USArrests,2,var)
pr.out=prcomp(USArrest, scale=TRUE)
pr.out=prcomp(USArrests, scale=TRUE)
pr.out$center
pr.out$scale
pr.out$rotation
biplot(pr.out, scale=0)
pr.var=pr.out$stdev^2
set.seed(2)
x=matrix(rnorm(50*2),ncol=2)
x[1:25,1]=x[1:25,1]+3
x[1:25, 2x[1:25]-4
x[1:25, 2]=x[1:25,2]-4
km.out=kmeans(x,2,nstart=2)
km.out=kmeans(x,2,nstart=20)
km.out$cluster
plot(x, col(km.out$cluster+1), main="K-Means Clustering Resultes with K=2", xlab="", ylab="", pch=20, cex=2)
plot(x, col=(km.out$cluster+1), main="K-Means Clustering Resultes with K=2", xlab="", ylab="", pch=20, cex=2)
set.seed(4)
km.out=kmeans(x,3, nstart=20)
km.out$cluster
km.out
hc.complete=hcluster(dist(x), method="complete")
hc.complete=hclust(dist(x), method="complete")
hc.averagee=hclust(dist(x), method="averagee")
hc.averagee=hclust(dist(x), method="average")
hc.single=hclust(dist(x), method="single")
par(mfrow=c(1,3))
plot(hc.complete, main="Complete linkage, xlab="", sub="", cex=0.9")
plot(hc.complete, main="Complete linkage", xlab="", sub="", cex=0.9)
plot(hc.completee, main="Averagee linkage", xlab="", sub="", cex=0.9)
plot(hc.averagee, main="Averagee linkage", xlab="", sub="", cex=0.9)
plot(hc.single, main="Single linkage", xlab="", sub="", cex=0.9)
cutree(hc.complete, 2)
cutree(hc.averagee, 2)
cutree(hc.single, 2)
library(AppliedPredictiveModeling)
data(concrete)
library(caret)
set.seed(1000)
inTrain = createDataPartition(mixtures$CompressiveStrength, p = 3/4)[[1]]
training = mixtures[ inTrain,]
testing = mixtures[-inTrain,]
range(training$SuperPlastizer)
names(training)
range(training$Superplasticizer)
summary(training$Superplasticizer)
smalltr<-training[,grepl("^IL", names(training))]
prep<-preProcess(smalltr, method="pca", thresh=0.8, outcome=training$diagnosis)
prep<-PreProcess(smalltr, method="pca", thresh=0.8, outcome=training$diagnosis)
prep<-preProcess(smalltr, method="pca", thresh=0.8, outcome=training$diagnosis)
prep<-preProcess(smalltr, method="pca", thresh=0.8, outcome=training$diagnosis)
prep<-preProcess(smalltr, method="pca",  outcome=training$diagnosis)
prep<-preProcess(smalltr, method="pca", thresh=0.8, training$diagnosis)
preProc <- preProcess(smalltr, method='pca', thresh=0.9,
outcome=training$diagnosis)
ss <- training[,grep('^IL', x = names(training) )]
View(ss)
View(training)
library(caret)
library(AppliedPredictiveModeling)
set.seed(3433)
data(AlzheimerDisease)
adData = data.frame(diagnosis,predictors)
inTrain = createDataPartition(adData$diagnosis, p = 3/4)[[1]]
training = adData[ inTrain,]
testing = adData[-inTrain,]
View(training)
ss <- training[,grep('^IL', x = names(training) )]
preProc <- preProcess(smalltr, method='pca', thresh=0.9,
outcome=training$diagnosis)
View(ss)
preProc <- preProcess(ss, method='pca', thresh=0.8,
outcome=training$diagnosis)
names(preproc)
names(preProc)
preProc$rotation
setwd("C:/Users/user/Dropbox/coursera/Relative-age-graphs")
knit("relative age.md")
library(knitr)
library(markdown)
knit("relative age.md")
knit("relative age.Rmd")
knit("relative age2.Rmd")
markdownToHTML("relative age2.md", "relative age 2.html")
knit("relative age2.Rmd")
markdownToHTML("relative age2.md", "relative age 2.html")
View(new)
View(new)
View(new)
write.table(new)
write.table(news2)
write.table(new2)
?write.table()
write.table(new, file="new.csv")
setwd("C:/Users/user/Dropbox/coursera/Relative-age-graphs/Relative-age-graphs")
write.table(new, file="new.csv")
write.table(new2, file="new2.csv")
new<-read.csv("new.csv")
View(new)
write.csv(new, "news.csv")
write.csv(new2, "new2.csv")
write.csv(new, "new.csv")
new<-read.csv("new.csv")
View(new)
new<-read.table("new.csv")
new<-dfgr2[13:24,]
dav(foo, file="new.Rda")
savw(foo, file="new.Rda")
save(foo, file="new.Rda")
save(new, file="new.Rda")
save(new2, file="new2.Rda")
load("new.Rda")
library(knit)
library(knitr)
library(markdown)
knit("relative age2.Rmd")
markdownToHTML("relative age2.md", "relative age2.html")
setwd("C:/Users/user/Dropbox/coursera/Relative-age-graphs/Relative-age-graphs")
knit("relative age2.Rmd")
